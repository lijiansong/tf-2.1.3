# Mixture of Experts

Mixture of Experts (MoE).

## REFs

- <https://github.com/eminorhan/mixture-of-experts>
- <https://severelytheoretical.wordpress.com/2018/06/08/the-softmax-bottleneck-is-a-special-case-of-a-more-general-phenomenon/>
- University of Toronto, Raquel Urtasun & Rich Zemel, CSC 411: Lecture 18: Ensemble Methods II, <https://www.cs.toronto.edu/~urtasun/courses/CSC411/18_mixture.pdf>
- CSC321, Introduction to Neural Networks and Machine Learning, Lecture 15: Mixtures of Experts, <https://www.cs.toronto.edu/~hinton/csc321/notes/lec15.pdf>
